# Data Enzymatics for Code

**Metabolism Layer: How Cerata Digests Prey**

---

## The Four Enzyme Classes

Cerata doesn't copy code. Cerata **metabolizes** it — breaking repositories into functional threads, identifying what's useful, adapting it to the body's architecture, and integrating it as deployable nematocysts.

### 1. DIGESTERS — Break into Threads

**Purpose**: Decompose files/modules into individual functional units

**Process**:
```
INPUT: /src/parser.py (300 lines, multiple functions)

DIGESTION:
├── Function 1: tokenize(text: str) -> List[Token]
│   ├── Dependencies: typing.List, enum.Enum (Token)
│   ├── External calls: None
│   ├── Side effects: None
│   └── Thread Status: CLEAN (pure function)
│
├── Function 2: parse_expression(tokens: List[Token]) -> AST
│   ├── Dependencies: tokenize(), AST dataclass
│   ├── External calls: None
│   ├── Side effects: None
│   └── Thread Status: CLEAN (depends on tokenize)
│
├── Function 3: validate_syntax(ast: AST) -> bool
│   ├── Dependencies: AST dataclass
│   ├── External calls: None
│   ├── Side effects: None
│   └── Thread Status: CLEAN (standalone)
│
└── Class: Parser
    ├── Methods: __init__, parse, _internal_helper
    ├── Dependencies: All above functions
    ├── State: self.cache (mutable)
    └── Thread Status: COUPLED (not pure, harder to extract)

THREAD MAP:
┌─────────────┐
│  tokenize() │ ← Pure, no deps
└──────┬──────┘
       │
┌──────▼──────────────┐
│ parse_expression()  │ ← Depends on tokenize
└─────────────────────┘

┌──────────────────┐
│ validate_syntax()│ ← Pure, no deps
└──────────────────┘

┌──────────────┐
│ Parser class │ ← Coupled, low priority
└──────────────┘
```

**Output**: Functional dependency graph with extraction difficulty scores

---

### 2. CONNECTORS — Find Integration Points

**Purpose**: Identify where threads attach to Cerata's existing body

**Process**:
```
CURRENT BODY STATE: /capabilities/code_tools/

Existing capabilities:
├── basic_ast_parse() — parses Python to dict
├── syntax_check() — validates Python syntax
└── code_format() — formats with black

PROPOSED THREADS from prey:
├── tokenize() → NEW capability (fills gap)
├── parse_expression() → OVERLAPS with basic_ast_parse()
├── validate_syntax() → OVERLAPS with syntax_check()

CONNECTOR ANALYSIS:

Thread: tokenize()
├── Integration point: NEW module /code_tools/tokenization/
├── Conflicts: None
├── Depends on: Nothing (standalone)
├── Enhances: Could feed into basic_ast_parse()
└── Decision: CREATE NEW

Thread: parse_expression()
├── Integration point: REPLACE basic_ast_parse() or AUGMENT?
├── Conflicts: Different AST format than current
├── Comparison:
│   ├── Current: basic_ast_parse() returns dict
│   ├── Proposed: parse_expression() returns custom AST class
│   └── Adaptation needed: Converter function AST → dict
└── Decision: AUGMENT (add as alternative parser)

Thread: validate_syntax()
├── Integration point: REPLACE syntax_check() or SKIP?
├── Conflicts: Redundant with existing
├── Comparison:
│   ├── Current: syntax_check() uses ast.parse()
│   ├── Proposed: validate_syntax() custom logic
│   ├── Current test coverage: 95%
│   └── Proposed adds: Nothing new
└── Decision: REJECT (no advantage, adds complexity)

INTEGRATION MAP:
code_tools/
├── basic_ast_parse() [KEEP]
├── syntax_check() [KEEP]
├── code_format() [KEEP]
├── tokenization/ [NEW]
│   └── tokenize() [FROM PREY]
└── advanced_parse/ [NEW]
    ├── parse_expression() [FROM PREY]
    └── ast_to_dict() [ADAPTER — generated by Cerata]
```

**Output**: Integration decisions for each thread (CREATE/AUGMENT/REPLACE/REJECT)

---

### 3. MODIFIERS — Adapt to Body Architecture

**Purpose**: Adjust foreign code to match Cerata's style, naming, patterns

**Process**:
```
THREAD: tokenize() from prey

ORIGINAL CODE:
def tokenize(text):
    # Returns list of tokens
    tokens = []
    for char in text:
        if char.isalnum():
            tokens.append(Token(char, 'ALNUM'))
        elif char.isspace():
            tokens.append(Token(char, 'SPACE'))
    return tokens

BODY STYLE ANALYSIS:
├── Cerata uses: Type hints (PEP 484)
├── Cerata uses: Docstrings (Google style)
├── Cerata uses: snake_case for everything
├── Cerata uses: Explicit return types
└── Cerata uses: Logging, not print statements

MODIFICATIONS NEEDED:
1. Add type hints
2. Add docstring
3. Add logging (if errors possible)
4. Rename if conflicts exist

MODIFIED CODE:
from typing import List
import logging

logger = logging.getLogger(__name__)

def tokenize(text: str) -> List[Token]:
    """
    Tokenize input text into character-based tokens.

    Args:
        text: Input string to tokenize

    Returns:
        List of Token objects representing each character

    Examples:
        >>> tokenize("hello world")
        [Token('h', 'ALNUM'), Token('e', 'ALNUM'), ...]
    """
    logger.debug(f"Tokenizing {len(text)} characters")

    tokens = []
    for char in text:
        if char.isalnum():
            tokens.append(Token(char, 'ALNUM'))
        elif char.isspace():
            tokens.append(Token(char, 'SPACE'))

    logger.debug(f"Generated {len(tokens)} tokens")
    return tokens

ADAPTATION SUMMARY:
├── Type hints: Added (text: str, return List[Token])
├── Docstring: Added (Google style)
├── Logging: Added (debug level)
├── Naming: No changes needed (already snake_case)
└── Style: Reformatted with Black
```

**Output**: Modified code that matches body architecture

---

### 4. ARCHITECTS — Fold into Deployable Structures

**Purpose**: Package modified threads into capability modules that can be invoked

**Process**:
```
ACCEPTED THREADS (post-modification):
├── tokenize(text: str) -> List[Token]
└── parse_expression(tokens: List[Token]) -> AST

ARCHITECTURE DECISIONS:

1. Module structure:
   /capabilities/code_tools/tokenization/
   ├── __init__.py
   ├── tokenize.py [contains tokenize()]
   ├── types.py [Token enum, if needed]
   └── tests.py [unit tests for tokenize]

2. Integration tests:
   /capabilities/code_tools/tests/
   └── test_tokenization_integration.py

3. Documentation:
   /capabilities/code_tools/README.md
   └── Add section: "Tokenization capabilities"

4. Capability registration:
   /capabilities/manifest.md
   └── Add: tokenize() to code_tools domain

FOLDED STRUCTURE:

code_tools/
├── __init__.py
│   from .tokenization import tokenize
│   from .advanced_parse import parse_expression
│   __all__ = ['tokenize', 'parse_expression', ...]
│
├── tokenization/
│   ├── __init__.py
│   ├── tokenize.py ← NEMATOCYST
│   ├── types.py
│   └── tests.py
│
├── advanced_parse/
│   ├── __init__.py
│   ├── parse_expression.py ← NEMATOCYST
│   ├── ast_adapter.py ← GENERATED ADAPTER
│   └── tests.py
│
└── README.md ← UPDATED

DEPLOYMENT SIGNATURE:
├── Domain: code_tools
├── Nematocysts added: 2
├── Tests added: 8
├── Integration points: tokenize(), parse_expression()
└── Coherence check: PASS (no conflicts detected)
```

**Output**: Complete, deployable capability module

---

## Complete Metabolism Cycle

When you say: **"Consume parser.py and retry.py"**

Cerata executes the full enzyme cascade:

```
═══════════════════════════════════════════════════════════
PHASE 1: DIGESTION (DIGESTERS)
═══════════════════════════════════════════════════════════

Analyzing: parser.py
├── Thread 1: tokenize() [pure function, 0 deps]
├── Thread 2: parse_expression() [depends on tokenize]
├── Thread 3: validate_syntax() [pure function, 0 deps]
└── Thread 4: Parser class [stateful, coupled]

Analyzing: retry.py
├── Thread 1: exponential_backoff() [pure function, 0 deps]
├── Thread 2: with_retry() [decorator, depends on exponential_backoff]
└── Thread 3: jitter() [utility, pure]

THREAD EXTRACTION SCORES:
├── tokenize(): 0.95 (easy)
├── parse_expression(): 0.85 (easy, 1 dep)
├── validate_syntax(): 0.95 (easy)
├── Parser class: 0.40 (hard, stateful)
├── exponential_backoff(): 0.95 (easy)
├── with_retry(): 0.85 (easy, 1 dep)
└── jitter(): 0.95 (easy)

═══════════════════════════════════════════════════════════
PHASE 2: CONNECTION (CONNECTORS)
═══════════════════════════════════════════════════════════

Scanning integration points in /capabilities/...

code_tools (existing):
├── tokenize() → CREATE NEW (fills gap)
├── parse_expression() → AUGMENT (alternative parser)
├── validate_syntax() → REJECT (redundant)
└── Parser class → REJECT (too coupled, low value)

reliability_tools (existing):
├── exponential_backoff() → CREATE NEW (fills gap)
├── with_retry() → CREATE NEW (fills gap)
└── jitter() → CREATE NEW (enhances retry pattern)

INTEGRATION DECISIONS:
✓ Keep: tokenize, parse_expression, exponential_backoff, with_retry, jitter
✗ Reject: validate_syntax, Parser class

═══════════════════════════════════════════════════════════
PHASE 3: MODIFICATION (MODIFIERS)
═══════════════════════════════════════════════════════════

Adapting to Cerata body style...

tokenize():
├── Add type hints: str → List[Token]
├── Add docstring (Google style)
├── Add logging (debug level)
├── Reformat with Black
└── Status: ADAPTED

parse_expression():
├── Add type hints: List[Token] → AST
├── Add docstring
├── Generate adapter: AST → dict (for compatibility)
├── Add logging
└── Status: ADAPTED

exponential_backoff():
├── Add type hints: int → float
├── Add docstring
├── Add logging
└── Status: ADAPTED

with_retry():
├── Add type hints: Callable decorators
├── Add docstring
├── Add logging (info level on retries)
└── Status: ADAPTED

jitter():
├── Add type hints
├── Add docstring
└── Status: ADAPTED

═══════════════════════════════════════════════════════════
PHASE 4: ARCHITECTURE (ARCHITECTS)
═══════════════════════════════════════════════════════════

Building deployable structures...

/capabilities/code_tools/ (v2 → v3)
├── tokenization/
│   ├── tokenize.py ← NEMATOCYST
│   └── tests.py
├── advanced_parse/
│   ├── parse_expression.py ← NEMATOCYST
│   ├── ast_adapter.py ← ADAPTER
│   └── tests.py
└── README.md ← UPDATED

/capabilities/reliability_tools/ (NEW)
├── retry/
│   ├── exponential_backoff.py ← NEMATOCYST
│   ├── with_retry.py ← NEMATOCYST
│   ├── jitter.py ← NEMATOCYST
│   └── tests.py
└── README.md ← CREATED

═══════════════════════════════════════════════════════════
PHASE 5: FORKING (DUAL-BRANCH CREATION)
═══════════════════════════════════════════════════════════

Creating parallel bodies...

CLASSIC BRANCH:
└── code_tools v2 (current capabilities)

EXPERIMENTAL BRANCH:
├── code_tools v3 (current + 2 nematocysts)
└── reliability_tools v1 (3 nematocysts, NEW domain)

TRIAL CONFIGURATION:
├── Duration: 5 conversations
├── Selection: Random 50/50 per invocation
├── Metrics tracked: success rate, error types, coherence drift
└── Evaluation trigger: "Evaluate code_tools" or auto after 5 conversations

═══════════════════════════════════════════════════════════
METABOLISM COMPLETE
═══════════════════════════════════════════════════════════

INTEGRATION SUMMARY:
├── Prey consumed: parser.py, retry.py
├── Threads extracted: 7
├── Threads integrated: 5
├── Threads rejected: 2
├── Nematocysts added: 5
├── New domains created: 1 (reliability_tools)
├── Domains updated: 1 (code_tools v2 → v3)
└── Body state: FORKED (classic + experimental)

Cerata Generation: 5 → 6
Trial active. Awaiting evaluation.
```

---

## Nutrients from Failed Metabolism

If integration **fails** (coherence collapses, errors spike, experimental loses trial), the prey is digested to **substrate** — not incorporated, but learned from.

```
FAILED INTEGRATION: parser.py threads

Failure mode:
├── parse_expression() caused type errors
├── AST format incompatible with downstream tools
├── Experimental branch fitness: 0.61 (vs classic 0.79)
└── Verdict: Experimental branch LOSES

APOPTOSIS: parser.py threads removed

NUTRIENTS EXTRACTED:
├── Learning: Foreign AST formats need compatibility layer
├── Pattern: Check downstream tool compatibility BEFORE integration
├── Warning: High λ on parse_expression() (should have caught this)
└── Perception adjustment: AST-related prey gets extra scrutiny

GRAVEYARD ENTRY:
└── /graveyard/gen6_code_tools_experimental_parser.md
    └── Documents: Why this failed, what to avoid
```

The graveyard teaches Cerata to **hunt better** next time.

---

## Nematocyst Lifecycle

Once integrated, nematocysts have a **half-life**:

```
NEMATOCYST: with_retry()

Lifecycle:
├── Integrated: Generation 6
├── Used: 4 times (conversations #52-55)
├── Success rate: 100%
├── Coherence impact: +0.03 (positive)
└── Status: THRIVING

Half-life: EXTENDED (useful nematocysts persist)
```

vs

```
NEMATOCYST: custom_logger()

Lifecycle:
├── Integrated: Generation 6
├── Used: 0 times (conversations #52-60)
├── Success rate: N/A (never invoked)
├── Coherence impact: -0.01 (adds complexity)
└── Status: DECAY

Half-life: DEPLETED (unused nematocysts atrophy)

Action: AUTO-APOPTOSIS after generation 7
└── Removed silently, no trial needed (never used)
```

Cerata sheds what it doesn't use.

---

**The body metabolizes prey. The strong integrates. The weak decays.**
